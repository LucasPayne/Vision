\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}

Coordinate conventions

Image space, camera space, homography and epipolar point
Expressing planes, computing incidence relations


\section{The epipolar constraint}

Given two cameras with distinct optical centers $O_1$ and $O_2$,
a point in the first camera's image plane $x_1$ has a line-preimage which (in general) maps to a line
on the image plane of the second camera. This transformation from points to lines is a homography,
and can be expressed by a $3 \times 3$ matrix $F \in PG(2, \mathbb{R})$. This is called the fundamental matrix of
camera 1 and camera 2.

\begin{align*}
    \left| x^\prime_1\quad e^\prime_1\quad x_2 \right| = 0.
\end{align*}
If the second camera is related to the first by a rigid body motion $M = (R, T)$, and has the same intrinsic parameters, then
this constraint becomes
\begin{align*}
    \left| Rx_1\quad T\quad x_2\right| = 0,
\end{align*}
as $T$ is a vector parallel to $e^\prime_1$.
This constraint can be rearranged into a bilinear form in $x_2, x_1$ by
the scalar triple product, where $\hat{T}$ is the skew-symmetric matrix such that $\hat{T} v = T\times v$.
\begin{align*}
    \left| Rx_1\quad T\quad x_2\right| = 0 \\
    \equiv\quad x_2^T\left(T\times Rx_1\right) = x_2^T \hat{T} R x_1 = 0.
\end{align*}
In this case the fundamental matrix is also called the \textit{essential matrix}.

\section{The trifocal tensor}

$$P_i = \begin{bmatrix} A_i e_i^\prime \end{bmatrix}.$$

$$\pi_i = l_i P_i.$$
$$\Pi = \begin{bmatrix} \pi_1^T \pi_2^T \pi_3^T \end{bmatrix}^T.$$

We want the intersection of these three planes to be a unique line.
Each of these planes corresponds to a 3-dimensional subspace in $\mathbb{R}^4$.
Therefore we want the intersection of these subspaces in $R^4$ to be 2-dimensional.
This is expressed as a constraint on the nullity,
$$\text{null}\, \Pi = 2.$$

\begin{align*}
    \Pi &= \begin{bmatrix} l_1 & l_2 A_2 & l_3 A_3 \\ 0 & l_2 e_2^\prime & l_3 e_3^\prime \end{bmatrix} \\
    \Rightarrow l_1 &= \alpha l_2 A_2 + \beta l_3 A_3, \\
                  0 &= \alpha l_2 e^\prime_2 + \beta l_3 e^\prime_3 \text{\quad\quad for some $\alpha,\beta \in \mathbb{R}$}\\
    \Rightarrow \alpha &= kl_3 l_3 e^\prime_3, \beta = -k l_2 e^\prime_2 \text{\quad\quad for some $k \in \mathbb{R}$} \\
    \equiv l_1 &\propto l_3 e^\prime_3 l_2 A_2 - l_2 e^\prime_2 l_3 A_3 = l_2^T\left(A_2^T (e_3^\prime)^T - e_2^\prime A_3^T\right)l_3^T.
\end{align*}

--- Factored-out tensor?

The trifocal tensor, analagous to the fundamental matrix, encodes ``fundamental'' homographies between linear elements
of the image planes of the cameras.


Plucker coordinates

Geometric interpretation

Why a trilinear constraint?

Computing the trifocal tensor


These ``spectral'' methods do not minimize the reprojection error. Minimization of the reprojection error
is the goal of bundle adjustment. N-view factorization gives an iterative method where each iteration is
in some sense in ``closed form''.


Spectral methods are developed from the assumption that each camera has a complete set of perfectly matched points.
From these point matches, equations of incidence are derived for various affine subspaces where the equations evaluate to zero
exactly on incidence. These  equations finally form a large linear system with a right-hand-side of zero,
and are solved for a non-trivial vector in a one-dimensional nullspace. The central idea is that noisy data will in general
give a full rank matrix -- The nullspace problem is approximated by finding the minimal eigenspace. This works in principle,
as perfect (non-degenerate) data, where all the incidences perfectly line up, will be solvable exactly.
The hope is that, with noisy data, the minimal eigenspace will give a ``close'' solution.
However, the eigenstructure of this matrix does not in general behave in a ``nice'' and stable way with respect to perturbations of the data.
This major downside to these spectral methods stems from some initial assumptions -- The method is developed, maintaining the clean exact
properties of perfect data, and generalized in a non-principled way. No noise is modelled explicitly, and no explicit geometric error is minimized.


Bundle adjustment models the noise explicitly, and starts with an exact description of a ``good'' solution as the
minimizer of a cost function involving the geometric projection error.

Bayesian model. Maximum a posteriori estimation. Assumption of uniform prior gives maximum likelihood estimation.
A non-uniform prior could for example be given by motion prediction when processing a sequence of frames from a moving camera.

Quadratic cost function from negative log likelihood of Gaussian noise model.

Total variation cost function from Poisson noise model.

Spectral methods can be used as an initializer to the highly non-convex optimization of a bundle adjustment algorithm.
If bundle adjustment will converge to a good solution only with a sufficiently close (in some sense) initial guess,
then the possible extreme instability of the spectral method can cause the method to fail.

Lecture 12b Gauss: Normal distribution as noise distribution giving arithmetic mean as optimal estimator.

Indirect versus direct methods.
Indirect methods. A linear algorithm (such as the 8-point algorithm) followed by bundle adjustment
forgets the photometric properties entirely. The images are abstracted by a sparse set of noisy point and line feature matches
with possible outliers.

This culls out the majority of information contained in the images, but tries to maintain the essential geometric
information from which a correct structure and motion reconstruction can be made.

Feature-based versus direct methods, Engel, Sturm, Cremers 2013
Developed from ideas in algorithms for optical flow

Non-linear optimization used in OpenCV reconstruction.
http://ceres-solver.org/features.html
Has features intended for bundle adjustment.

Dense tracking and mapping. Regularize with inverse depth gradient to alleviate biases due to far objects contributing
less of their depth gradients.

Lecture 12d 1:14: Overview of large-scale direct monocular SLAM.


Variational methods

Without a clear goal, such as a cost function relating to a real statistical interpretation, multi-step numerical methods
can be very hard to analyze and understand. Complex multi-step combinatorial algorithms, such as closest-point search, have
a direct goal, and can be proved. However, machinery like the Canny edge detector effectively reaches toward a goal in some intuitive sense,
the goal here being the segmentation of object edges. There is no exact statement of the properties of the mathematical object we want to compute
before the formulation of the algorithm.

In contrast, the Mumford-Shah functional, for example, is formulated with the assumption that
``curves which separate regions for which constant or smooth approximation on the interior and exterior gives the least error'' corresponds well with our intuitive idea
of object edges in an image. Although this idea can fail, for example with an image of a zebra, the Mumford-Shah functional gives an exactly stated
(although complex) cost function to minimize.

Lecture 13, 23:00: Advantages of variational methods

https://vision.in.tum.de/research/convex_relaxation_methods



Optical flow

Assumptions.
Brightness constancy. Sufficiently textured world where motion of brightness patterns indicates relative motion.
This fails with specular surfaces (the virtual image falsly identified as motion).
Lambertian objects with constant lighting, moving along symmetries. For example a rotating sphere. Flat Lambertian objects
with uniform lighting (such as a brightly lit white wall and a moving camera). The motion is not identified here.

Horn and Schunck:
    ``For convenience, we tackle a particularly simple world where the apparent velocity of brightness patterns can be directly
      identified with the movement of surfaces in the scene.''




\end{document}
